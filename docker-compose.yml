version: "3.8"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - namenode_data:/hadoop/dfs/name
      - ./docker_data:/data
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9000"]
      interval: 10s
      timeout: 5s
      retries: 10

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - SERVICE_PRECONDITION=namenode:9000
    depends_on:
      namenode:
        condition: service_healthy
    ports:
      - "9864:9864"
    volumes:
      - datanode_data:/hadoop/dfs/data

  spark:
    build:
      context: .
      dockerfile: spark.Dockerfile
    container_name: spark
    user: root
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - SPARK_CONF_spark_eventLog_enabled=true
      - SPARK_CONF_spark_eventLog_dir=hdfs://namenode:9000/spark-logs
      - SPARK_CONF_spark_history_fs_logDirectory=hdfs://namenode:9000/spark-logs
    depends_on:
      namenode:
        condition: service_healthy
    ports:
      - "4040:4040"
      - "8080:8080"
      - "18080:18080"
    volumes:
      - ./docker_data:/data

volumes:
  namenode_data:
  datanode_data:
