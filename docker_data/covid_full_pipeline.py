from pyspark.sql import SparkSession, Row, functions as F, types as T
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler, StringIndexer

def main():
    spark = SparkSession.builder \
        .appName("COVID19 Full Pipeline") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
        .enableHiveSupport() \
        .getOrCreate()

    # ============================================================
    # –®–∞–≥ 1 ‚Äî –ß—Ç–µ–Ω–∏–µ CSV
    # ============================================================

    df = spark.read \
        .option("header", True) \
        .option("inferSchema", True) \
        .csv("hdfs://namenode:9000/covid_dataset/metadata/metadata_cleaned.csv")

    print("‚úÖ CSV –∑–∞–≥—Ä—É–∂–µ–Ω:")
    df.show(5)

    # ============================================================
    # –®–∞–≥ 2 ‚Äî –ó–∞–ø–∏—Å—å —Å –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –±–∞–∫–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    # ============================================================

    # Partition –ø–æ finding_grouped, age_group
    # Bucket –ø–æ sex –∏ view
    df.write \
        .mode("overwrite") \
        .partitionBy("finding_grouped", "age_group") \
        .bucketBy(8, "sex", "view") \
        .sortBy("age") \
        .format("parquet") \
        .saveAsTable("covid_metadata_partitioned")

    print("‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ parquet-—Ç–∞–±–ª–∏—Ü—É —Å –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –±–∞–∫–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º.")

    # ============================================================
    # –®–∞–≥ 3 ‚Äî SQL-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞ (3 –∑–∞–ø—Ä–æ—Å–∞)
    # ============================================================

    # –û–∫–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    query1 = spark.sql("""
        SELECT
            patientid,
            finding_grouped,
            age,
            ROW_NUMBER() OVER (
                PARTITION BY finding_grouped
                ORDER BY age DESC
            ) as rank
        FROM covid_metadata_partitioned
    """)
    query1.show(5)

    query1.write.mode("overwrite").parquet(
        "hdfs://namenode:9000/covid_dataset/processed/query1_top_patients.parquet"
    )
    print("‚úÖ –û–∫–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω.")

    # –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –ø—Ä–æ–µ–∫—Ü–∏–π —Å–Ω–∏–º–∫–æ–≤
    views = [
        Row(view="PA", view_desc="Posteroanterior view"),
        Row(view="AP", view_desc="Anteroposterior view"),
        Row(view="LATERAL", view_desc="Lateral view")
    ]
    views_df = spark.createDataFrame(views)
    views_df.createOrReplaceTempView("view_lookup")

    # JOIN metadata + view_lookup
    query2 = spark.sql("""
        SELECT m.patientid, m.finding_grouped, m.view, v.view_desc
        FROM covid_metadata_partitioned m
        LEFT JOIN view_lookup v
        ON m.view = v.view
    """)
    query2.show(5)

    query2.write.mode("overwrite").parquet(
        "hdfs://namenode:9000/covid_dataset/processed/query2_views_join.parquet"
    )
    print("‚úÖ JOIN –≤—ã–ø–æ–ª–Ω–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω.")

    # –ü–æ–¥–∑–∞–ø—Ä–æ—Å –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–∞ COVID-19
    query3 = spark.sql("""
        SELECT
            'COVID-19' as diagnosis,
            covid_count,
            total_count,
            ROUND(covid_count / total_count * 100, 2) as covid_percent
        FROM (
            SELECT
                SUM(CASE WHEN finding_grouped = 'COVID-19' THEN 1 ELSE 0 END) as covid_count,
                COUNT(*) as total_count
            FROM covid_metadata_partitioned
        )
    """)
    query3.show()

    query3.write.mode("overwrite").parquet(
        "hdfs://namenode:9000/covid_dataset/processed/query3_covid_stats.parquet"
    )
    print("‚úÖ –ü–æ–¥–∑–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω.")

    # ============================================================
    # –®–∞–≥ 4 ‚Äî PySpark-–æ–±—Ä–∞–±–æ—Ç–∫–∞
    # ============================================================

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ COVID-19
    covid_df = df.filter(df["finding_grouped"] == "COVID-19")
    covid_df.show(5)

    covid_df.write.mode("overwrite").parquet(
        "hdfs://namenode:9000/covid_dataset/processed/covid_only.parquet"
    )
    print("‚úÖ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ COVID-19 –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –∏ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")

    # ------------------------------------------------------------
    # UDF ‚Äî –£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–∏–∞–≥–Ω–æ–∑–æ–≤
    # ------------------------------------------------------------

    @F.udf(returnType=T.StringType())
    def unify_diagnosis(finding):
        if finding and "covid" in finding.lower():
            return "COVID-19"
        elif finding and "pneumonia" in finding.lower():
            return "PNEUMONIA"
        else:
            return "OTHER"

    df_udf = df.withColumn("finding_unified", unify_diagnosis(F.col("finding")))
    df_udf.show(5)

    # ------------------------------------------------------------
    # UDF ‚Äî –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –≤–æ–∑—Ä–∞—Å—Ç–∞
    # ------------------------------------------------------------

    @F.udf(returnType=T.StringType())
    def age_category(age):
        if age is None:
            return "unknown"
        elif age < 30:
            return "young"
        elif age <= 60:
            return "middle"
        else:
            return "old"

    df_udf = df_udf.withColumn("age_category", age_category(F.col("age")))
    df_udf.show(5)

    df_udf.write.mode("overwrite").parquet(
        "hdfs://namenode:9000/covid_dataset/processed/metadata_with_udf.parquet"
    )
    print("‚úÖ UDF-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –∏ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")

    # ------------------------------------------------------------
    # ML ‚Äî –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è KMeans
    # ------------------------------------------------------------

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è ML
    indexer = StringIndexer(inputCol="sex", outputCol="sex_index", handleInvalid="keep")
    df_ml = indexer.fit(df_udf).transform(df_udf)

    assembler = VectorAssembler(
        inputCols=["age", "sex_index"],
        outputCol="features"
    )
    df_ml = assembler.transform(df_ml).na.drop(subset=["features"])

    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    kmeans = KMeans(k=3, seed=1)
    model = kmeans.fit(df_ml)
    predictions = model.transform(df_ml)

    predictions.select("patientid", "age", "sex", "prediction").show(5)

    predictions.write.mode("overwrite").parquet(
        "hdfs://namenode:9000/covid_dataset/processed/kmeans_clusters.parquet"
    )
    print("‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")

    # ------------------------------------------------------------
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ metadata.parquet
    # ------------------------------------------------------------

    df_udf.write.mode("overwrite").parquet(
        "hdfs://namenode:9000/covid_dataset/metadata/metadata.parquet"
    )
    print("‚úÖ metadata.parquet —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ HDFS.")

    spark.stop()
    print("üéâ –í—Å–µ —ç—Ç–∞–ø—ã —Å–∫—Ä–∏–ø—Ç–∞ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã.")

if __name__ == "__main__":
    main()
